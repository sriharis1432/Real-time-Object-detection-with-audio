# -*- coding: utf-8 -*-
"""object _detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18MKcF70q9BgURPsYy8xdY7W7eDyAH2m7

# For vedio --> object detection with audio
"""

# !pip install opencv-contrib-python # some people ask the difference between this and opencv-python and opencv-python contains the main packages wheras the other
                                    # contains both main modules and contrib/extra modules
# !pip install cvlib               # for object detection
# !pip install gtts                # convert text to speech
# !pip install playsound           # generate the sound for file
# !pip3 install PyObjC`            # if you want playsound to run more efficiently.

import cv2
import cvlib as cv
from cvlib.object_detection import draw_bbox
from gtts import gTTS
from playsound import playsound

def speech(text):
    print(text)
    language = "en"
    output = gTTS(text=text, lang=language, slow=False)

    output.save(".output.mp3")
    playsound("output.mp3")


video = cv2.VideoCapture('/content/DOC-20220501-WA0041.')
labels = []

while True:
    ret, frame = video.read()

    if not ret:
      print('error: filed to grab frame')
      break
    # Bounding box.
    # the cvlib library has learned some basic objects using object learning
    # usually it takes around 800 images for it to learn what a phone is.
    bbox, label, conf = cv.detect_common_objects(frame)  # Detects common objects in the frame, returning bounding boxes, labels, and confidence scores.

    output_image = draw_bbox(frame, bbox, label, conf)   # Draws bounding boxes around detected objects and labels them.

    cv2.imshow("Detection", output_image)

    for item in label:
        if item in labels:
            pass
        else:
            labels.append(item)

    if cv2.waitKey(1) & 0xFF == ord("q"):    # Breaks the loop if 'q' is pressed.
        break

i = 0
new_sentence = []
for label in labels:
    if i == 0:
        new_sentence.append(f"I found a {label}, and, ")
    else:
        new_sentence.append(f"a {label},")

    i += 1

speech(" ".join(new_sentence))

import cv2
import cvlib as cv
from cvlib.object_detection import draw_bbox
from gtts import gTTS
from playsound import playsound

def speech(text):
  for i,text in enumerate(text):
    print(text)
    language = "en"
    output = gTTS(text=text, lang=language, slow=False)
    output.save(f"{i}.mp3")

text=['man is good and women is bad','mouns is good']
speech(text)

import cv2
import cvlib as cv
from cvlib.object_detection import draw_bbox
from gtts import gTTS
from playsound import playsound

def speech(text):
    print(text)
    language = "en"
    output = gTTS(text=text, lang=language, slow=False)
    output.save("output.mp3")
    playsound("output.mp3")

# Paths to YOLO files
config_path = '/content/yolov3.cfg'
weights_path = '/content/yolov3 (1).weights'
names_path = '/content/coco.names'

# Load YOLO
net = cv2.dnn.readNetFromDarknet(config_path, weights_path)

# image=cv2.imread('/content/117kb.jpg')
video = cv2.VideoCapture(0)
labels = []

while True:
    ret, frame = video.read()
    if not ret:
        print("Failed to grab frame")
        break

    # Detect common objects in the frame
    bbox, label, conf = cv.detect_common_objects(frame, model='yolov3', enable_gpu=False)

    output_image = draw_bbox(frame, bbox, label, conf)
    cv2.imshow("Detection", output_image)

    for item in label:
        if item not in labels:
            labels.append(item)

    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

# video.release()
# cv2.destroyAllWindows()

i = 0
new_sentence = []
for label in labels:
    if i == 0:
        new_sentence.append(f"I found a {label}, and, ")
    else:
        new_sentence.append(f"a {label},")
    i += 1

# speech(" ".join(new_sentence))

"""# For Image, --> object detection and audio"""

import cv2

video = cv2.VideoCapture(0)  # Use 0 for the default webcam
while True:
    ret, frame = video.read()
    if not ret:
        print("Failed to grab frame")
        break
    cv2.imshow("Frame", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'): # Exit on 'q' key press
        break

# Release the video capture object and close all OpenCV windows
video.release()
cv2.destroyAllWindows()

# Required installations
# pip install opencv-contrib-python
#!pip install cvlib
# !pip install gtts
# !pip install playsound

import cv2
import cvlib as cv
from cvlib.object_detection import draw_bbox
from gtts import gTTS
from playsound import playsound

def speech(text):
    print(text)
    language = "en"
    output = gTTS(text=text, lang=language, slow=False)
    output.save("output.mp3")
    playsound("output.mp3")

# Path to the image file
image_path = "/content/117kb.jpg"

# Load the image
image = cv2.imread(image_path)
labels = []

# Bounding box detection
bbox, label, conf = cv.detect_common_objects(image) # Detects common objects in the frame, returning bounding boxes, labels, and confidence scores.

# Draw bounding boxes on the image
output_image = draw_bbox(image, bbox, label, conf)

# Display the image with bounding boxes
cv2.imshow("Detection", output_image)
cv2.waitKey(0)  # Wait for a key press to close the image window
cv2.destroyAllWindows()

# Collect unique labels
for item in label:
    if item in labels:
        pass
    else:
        labels.append(item)

# Generate and play the speech description
i = 0
new_sentence = []
for label in labels:
    if i == 0:
        new_sentence.append(f"I found a {label}, and, ")
    else:
        new_sentence.append(f"a {label},")
    i += 1

speech(" ".join(new_sentence))